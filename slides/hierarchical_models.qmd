---
title: "Hierarchical Models"
date: "September 17, 2024"
date-modified: last-modified
format: metropolis-revealjs
---

```{r}
#| include: false
#| file: _setting.R
```

# Therapeutic Touch Example (*N* = 28)

## Data Points From One Person

:::: {.columns}

::: {.column width="40%"}

![](https://c.pxhere.com/photos/6b/f4/protect_hands_energy_ecology_protection_sun_live_responsibility-1334144.jpg!d)

$y$: whether the guess of which hand was hovered over was correct

:::

::: {.column width="60%"}

Person S01

```{r}
tt_url <- paste0(
    "https://github.com/boboppie/kruschke-doing_bayesian_data_analysis/",
    "raw/master/2e/TherapeuticTouchData.csv"
)
tt_dat <- read.csv(tt_url)
# Get aggregated data by summing the counts
knitr::kable(tt_dat[tt_dat$s == "S01", ])
```

:::

::::

## Binomial Model

We can use a Bernoulli model:
$$
y_i \sim \mathrm{Bern}(\theta)
$$
for $i = 1, \ldots, N$

. . .

Assuming exchangeability given $\theta$, more succint to write
$$
z \sim \mathrm{Bin}(N, \theta)
$$
for $z = \sum_{i = 1}^N y_i$

. . .

- Bernoulli: Individual trial
- Binomial: total count of "1"s

---

Prior: Beta(1, 1)

1 success, 9 failures

Posterior: Beta(2, 10)

```{r}
#| fig-width: 5
#| fig-asp: .618
#| fig-align: center
#| out-width: "90%"
ggplot() +
    stat_function(fun = dbeta, args = list(shape1 = 2, shape2 = 10)) +
    labs(x = expression(theta), y = "Posterior density")
```

## Multiple People

:::: {.columns}

::: {.column width="50%"}

```{r}
#| fig-width: 4.5
#| fig-asp: 0.618
# Get aggregated data by summing the counts
tt_agg <- tt_dat |>
    group_by(s) |>
    summarise(y = sum(y),  # total number of correct
              n = n())
# Plot proportion correct distribution
ggplot(tt_agg, aes(x = y / n)) +
    geom_histogram(binwidth = .1) +
    labs(x = "Proportion Correct")
```

:::

::: {.column width="50%"}

We could repeat the binomial model for each of the 28 participants, to obtain posteriors for $\theta_1$, $\ldots$, $\theta_{28}$

:::

::::

But . . . 

::: {.notes}
We'll continue the therapeutic touch example. To recap, we have 28 participants, each of them go through 10 trials to guess which of their hands was hovered above. The histogram shows the distribution of the proportion correct.
:::

. . .

> Do we think our belief about $\theta_1$ would inform our belief about $\theta_2$, etc?

. . .

After all, human beings share 99.9% of genetic makeup

## Three Positions of Pooling

- No pooling: each individual is completely different; inference of $\theta_1$ should be independent of $\theta_2$, etc

- Complete pooling: each individual is exactly the same; just one $\theta$ instead of 28 $\theta_j$'s

- **Partial pooling**: each individual has something in common but also is somewhat different

## No Pooling

```{dot}
//| fig-align: center
//| fig-asp: 0.8
//| out-width: 100%
digraph nopool {
  layout=neato

  node [penwidth = 0, fontname = "Ubuntu"]

  # Person
  th1 [pos = "-1.2,1!", label=<&theta;<SUB>1</SUB>>];
  th2 [pos = "-0.6,1!", label=<&theta;<SUB>2</SUB>>]
  th3 [pos = "0,1!", label="..."]
  th4 [pos = "0.6,1!", label=<&theta;<SUB>J - 1</SUB>>]
  th5 [pos = "1.2,1!", label=<&theta;<SUB>J</SUB>>]

  # Repeated measures
  y1 [pos = "-1.2,0!", label=<y<SUB>1</SUB>>]
  y2 [pos = "-0.6,0!", label=<y<SUB>2</SUB>>]
  y3 [pos = "0,0!", label="..."]
  y4 [pos = "0.6,0!", label=<y<SUB>J - 1</SUB>>]
  y5 [pos = "1.2,0!", label=<y<SUB>J</SUB>>]

  # edges
  edge [dir = "none"]
  th1 -> y1
  th2 -> y2
  th4 -> y4
  th5 -> y5
}
```

## Complete Pooling

```{dot}
//| fig-align: center
//| fig-asp: 0.8
//| out-width: 100%
digraph completepool {
  layout=neato

  node [penwidth = 0, fontname = "Ubuntu"]

  # Person
  th [pos = "0,1!", label=<&theta;>];

  # Repeated measures
  y1 [pos = "-1.2,0!", label=<y<SUB>1</SUB>>]
  y2 [pos = "-0.6,0!", label=<y<SUB>2</SUB>>]
  y3 [pos = "0,0!", label="..."]
  y4 [pos = "0.6,0!", label=<y<SUB>J - 1</SUB>>]
  y5 [pos = "1.2,0!", label=<y<SUB>J</SUB>>]

  # edges
  edge [dir = "none"]
  th -> {y1; y2; y4; y5}
}
```

## Partial Pooling

```{dot}
//| fig-align: center
//| fig-asp: 0.5
//| fig-height: 6
digraph partialpool {
  layout=neato

  node [penwidth = 0, fontname = "Ubuntu"]

  # Common parameters
  hy [pos = "0,2!", label=<&mu;, &kappa;>]

  # Person
  th1 [pos = "-1.2,1!", label=<&theta;<SUB>1</SUB>>];
  th2 [pos = "-0.6,1!", label=<&theta;<SUB>2</SUB>>]
  th3 [pos = "0,1!", label="..."]
  th4 [pos = "0.6,1!", label=<&theta;<SUB>J - 1</SUB>>]
  th5 [pos = "1.2,1!", label=<&theta;<SUB>J</SUB>>]

  # Repeated measures
  y1 [pos = "-1.2,0!", label=<y<SUB>1</SUB>>]
  y2 [pos = "-0.6,0!", label=<y<SUB>2</SUB>>]
  y3 [pos = "0,0!", label="..."]
  y4 [pos = "0.6,0!", label=<y<SUB>J - 1</SUB>>]
  y5 [pos = "1.2,0!", label=<y<SUB>J</SUB>>]

  # edges
  edge [dir = "none"]
  hy -> {th1; th2; th4; th5;}
  th1 -> y1
  th2 -> y2
  th4 -> y4
  th5 -> y5
}
```

## Partial Pooling in Hierarchical Models

Hierarchical Priors: $\theta_j \sim \mathrm{Beta2}(\mu, \kappa)$

Beta2: *reparameterized* Beta distribution

- mean $\mu = a / (a + b)$
- concentration $\kappa = a + b$

Expresses the prior belief:

> Individual $\theta$s follow a common Beta distribution with mean $\mu$ and concentration $\kappa$

## How to Choose $\kappa$

If $\kappa \to \infty$: everyone is the same; no individual differences (i.e., complete pooling)

If $\kappa = 0$: everybody is different; nothing is shared (i.e., no pooling)

. . .

We can fix a $\kappa$ value based on our belief of how individuals are similar or different

. . .

A more Bayesian approach is to treat $\kappa$ as an unknown, and use Bayesian inference to update our belief about $\kappa$

---

Generic prior by Kruschke (2015): $\kappa$ $\sim$ Gamma(0.01, 0.01) 

```{r}
#| fig-width: 5
#| fig-asp: 0.5
ggplot() +
    stat_function(fun = dgamma,
                  args = list(shape = 0.01, rate = 0.01),
                  n = 501, aes(col = "Gamma(0.01, 0.01)")) +
    stat_function(fun = dgamma,
                  args = list(shape = 0.1, rate = 0.1),
                  n = 501, aes(col = "Gamma(0.1, 0.1)")) +
    stat_function(fun = dgamma,
                  args = list(shape = 1, rate = 1),
                  n = 501, aes(col = "Gamma(1, 1)")) +
    labs(x = expression(kappa), y = "density") +
    xlim(0, 10)
```

Sometimes you may want a stronger prior like Gamma(1, 1), if it is unrealistic to do no pooling

## Full Model

Model:
$$
\begin{aligned}
  z_j & \sim \mathrm{Bin}(N_j, \theta_j) \\
  \theta_j & \sim \mathrm{Beta2}(\mu, \kappa)
\end{aligned}
$$
Prior:
$$
\begin{aligned}
  \mu & \sim \mathrm{Beta}(1.5, 1.5) \\
  \kappa & \sim \mathrm{Gamma}(0.1, 0.1)
\end{aligned}
$$

::: {.notes}
Here's our model. For each person j, the number of correct responses follows a binoial distribution in N equals 10 trials. The probability, or the ability to sense the experimenter's hand, is captured by theta j.

Now, we assume the theta js come from a common Beta distribution. Beta here is parameterized with the common mean mu, and the concentration kappa that controls how much to pool.

As for the priors, because mu is the common mean of the thetas, it is again between 0 and 1. I set a weak Beta(1.5, 1.5) prior, which corresponds to half a success and half a failure. If you have stronger belief that the data mostly reflect random guessing, you can put something like Beta(5, 5).

The gamma prior for kappa is recommended by your text as a weak default prior.
:::

---

```{stan}
#| echo: true
#| output.var: hbin_mod
#| file: "../usc-psyc573-notes/stan_code/hierarchical-binomial.stan"
#| eval: false
```

::: {.notes}
Here is the Stan code. The inputs are J, the number of people, y, which is actually z in our model for the individual counts, but I use y just because y is usually the outcome in Stan. N is the number of trials per person, and here N[J] means the number of trials can be different across individuals.

The parameters and the model block pretty much follow the mathematical model. The beta_proportion() function is what I said Beta2 as the beta distribution with the mean and the concentration as the parameters.

You may want to pause here to make sure you understand the Stan code.
:::

---

```r
hbin_mod <- cmdstan_model("stan_code/hierarchical-binomial.stan")
```

```r
tt_fit <- hbin_mod$sample(
    data = list(J = nrow(tt_agg),
                y = tt_agg$y,
                N = tt_agg$n),
    seed = 1716,  # for reproducibility
    refresh = 1000
)
```

## Posterior of Hyperparameters

```{r}
tt_fit <- readRDS("../usc-psyc573-notes/tt_fit.RDS")
```

```{r}
#| fig-width: 7
#| fig-asp: 0.5
#| echo: true
library(bayesplot)
tt_fit$draws(c("mu", "kappa")) |>
    mcmc_dens()
```

::: {.notes}
The graphs show the posterior for mu and kappa. As you can see, the average probability of guessing correctly has most density between .4 and .5.

For kappa, the posterior has a pretty long tail, and the value of kappa being very large, like 100 or 200, is pretty likely. So this suggests the individuals may be pretty similar to each other.
:::

## Shrinkage

```{r}
#| layout-nrow: 2
#| fig-width: 4
#| fig-height: 2.5
#| fig-align: center
theta1_post <- tt_fit$draws("theta[1]", format = "draws_df")
ggplot() +
    stat_function(
        fun = dbeta, args = list(shape1 = 2, shape2 = 10),
        aes(col = "no pooling")
    ) +
    geom_density(
        data = theta1_post,
        aes(x = `theta[1]`, col = "partial pooling"), bw = "SJ"
    ) +
    labs(
        x = expression(theta[1]),
        y = "Posterior density", col = NULL
    ) +
    xlim(0, 1)
theta15_post <- tt_fit$draws("theta[15]", format = "draws_df")
ggplot() +
    stat_function(
        fun = dbeta, args = list(shape1 = 2, shape2 = 10),
        aes(col = "no pooling")
    ) +
    geom_density(
        data = theta15_post,
        aes(x = `theta[15]`, col = "partial pooling"), bw = "SJ"
    ) +
    labs(
        x = expression(theta[15]),
        y = "Posterior density", col = NULL
    ) +
    xlim(0, 1)
```

::: {.notes}
From the previous model, we get posterior distributions for all parameters, including mu, kappa, and 28 thetas. The first graph shows the posterior for theta for person 1. The red curve is the one without any pooling, so the distribution is purely based on the 10 trials for person 1. The blue curve, on the other hand, is much closer to .5 due to partial pooling. Because the posterior of kappa is pretty large, the posterior is pooled towards the grand mean, mu.

For the graph below, the posterior mean is close to .5 with or without partial pooling, but the distribution is narrower with partial pooling, which reflects a stronger belief. This is because, with partial pooling, the posterior distribution uses more information than just the 10 trials of person 15; it also borrows information from the other 27 individuals.
:::

## Multiple Comparisons?

Frequentist: family-wise error rate depends on the number of intended contrasts

::: {.notes}

One advantage of the hierarchical model is it is a solution to the multiple comparison problem. In frequentist analysis, if you have multiple groups, and you want to test each contrast, you will need to consider family-wise error rate, and do something like Bonferroni corrections.
:::

. . .

Bayesian: only one posterior; hierarchical priors already express the possibility that groups are the same

::: {.notes}
The Bayesian alternative is to do a hierarchial model with partial pooling. With Bayesian, you have one posterior distribution, which is the joint distribution of all parameters. And the use of a common distribution of the thetas already assigns some probability to the prior belief that the groups are the same.
:::

. . .

> Thus, Bayesian hierarchical model "completely solves the multiple comparisons problem."^[see <https://statmodeling.stat.columbia.edu/2016/08/22/bayesian-inference-completely-solves-the-multiple-comparisons-problem/>]

::: {.notes}
Therefore, with a hierarchical model, you can obtain the posterior of the difference of any groups, without worrying about how many comparisons you have conducted. You can read more in the sources listed here.
:::

# Hierarchical Normal Model

::: {.notes}
In this video, we'll talk about another Bayesian hierarchical model, the hierarchical normal model.
:::

## Effect of coaching on SAT-V

```{r}
schools_dat <- list(J = 8,
                    y = c(28, 8, -3,  7, -1, 1, 18, 12),
                    sigma = c(15, 10, 16, 11, 9, 11, 10, 18))
with(schools_dat,
     data.frame(School = LETTERS[1:8],
                `Treatment Effect Estimate` = y,
                `Standard Error` = sigma)) |>
    knitr::kable()
```

::: {.notes}
The data come from the 1980s when scholars were debating the effect of coaching on standardized tests. The test of interest is the SAT verbal subtest. The note contains more description of it.

The analysis will be on the secondary data from eight schools, from school A to school H. Each schools conducts its own randomized trial. The middle column shows the treatment effect estimate for the effect of coaching. For example, for school A, we see that students with coaching outperformed students without coaching by 28 points. However, for schools C and E, the effects were smaller and negative. 

Finally, in the last column, we have the standard error of the treatment effect for each school, based on a t-test. As you know, the smaller the standard error, the less uncertainty we have on the treatment effect.
:::

---

Model:
$$
\begin{aligned}
  d_j & \sim N(\theta_j, s_j) \\
  \theta_j & \sim N(\mu, \tau)
\end{aligned}
$$
Prior:
$$
\begin{aligned}
  \mu & \sim N(0, 100) \\
  \tau & \sim t^+_4(0, 100)
\end{aligned}
$$

::: {.notes}
We can use the same idea of partial pooling for this data. The idea is that, while the effect of coaching may be different across schools, there should be some similarity of the schools. Like if you are a school official, if you hear all other schools found coaching to be increasing performance, you'd probably expect coaching works for the students in your school as well.

So the logic is the same; but instead of a binomial outcome, we have something like a continuous outcome in treatment effect, so we model the treatment effect, which I call d here, by a normal distribution. Note that d here is the sample difference between the treatment and the control group; because the sample difference is not the true treatment effect, we assume d j is normally distributed with a mean theta j, where theta j is the true treatment effect for school j. s j here is the standard error of the treatment effect, the third column in the data. It reflects the degree of uncertainty in the sample treatment effect d. 

Next, we have the theta js coming from a common normal distribution, with mean mu, and standard deviation tau. So like kappa in the previous model, tau here controls how much to pool. If tau is small, it means the thetas are very similar; if tau is large, it means the thetas are very different.
:::

---

```{stan}
#| echo: true
#| output.var: hnorm_mod
#| file: "../usc-psyc573-notes/stan_code/hierarchical-normal.stan"
#| eval: false
```

## Individual-School Treatment Effects

```{r}
#| results: hide
#| eval: false
fit <- hnorm_mod$sample(
    data = schools_dat,
    seed = 1804,  # for reproducibility
    refresh = 1000
)
```

```{r}
fit <- readRDS("../usc-psyc573-notes/hnorm_mod.RDS")
```

```{r}
#| fig-width: 5
#| fig-asp: .618
#| fig-align: center
fit$draws("theta") |>
    mcmc_areas()
```

## Prediction Interval

Posterior distribution of the true effect size of a new study, $\tilde \theta$

```{r}
#| fig-width: 4.5
#| fig-asp: .618
#| out-width: 60%
#| fig-align: center
library(posterior)
# Prediction Interval (can also be done in Stan)
fit$draws(c("mu", "tau")) |>
    as_draws_array() |>
    mutate_variables(theta_tilde = rnorm(4000, mean = mu, sd = tau)) |>
    mcmc_areas()
```

::: aside
See <https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.12> for an introductory paper on random-effect meta-analysis
:::