{
  "hash": "4a57e6c15c03424f79ca0ae6c6216122",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Markov Chain Monte Carlo\"\ndate-modified: last-modified\nformat: metropolis-revealjs\n---\n\n\n\n\n\n\n::: {.content-hidden unless-profile=\"class\"}\n\n## {.smaller}\n\n|                       | Bayesian Analysis                          | Frequentist Analysis                             |\n|-----------------------|--------------------------------------------|--------------------------------------------------|\n| Use of probability    | Uncertainty in parameters                  | Sampling error                                   |\n| Inferences            | Posterior distribution of parameters       | Sampling distribution of data statistics         |\n| Information used      | Prior + data + model/assumptions           | Data + model/assumptions                         |\n| Estimation            | Often MCMC                                 | Least squares, maximum lieklihood, resampling    |\n| Statistics to report  | Posterior distribution, posterior mean, posterior SD, credible interval | MLE, SE, $p$-value, confidence interval |\n| Model Comparison      | DIC/WAIC/LOO, Bayes factors                | AIC, Likelihood ratio tests                      |\n| Workflow              | Similar for different models               | Usually specific to particular tests             |\n\n## Example: $t$-test (Frequentist)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndatfile <- here::here(\"data_files\", \"ENDFILE.xlsx\")\nlies <- readxl::read_excel(datfile)\nt.test(lies$LDMRT / 1000, lies$LEMRT / 1000, paired = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPaired t-test\n\ndata:  lies$LDMRT/1000 and lies$LEMRT/1000\nt = 0.7, df = 62, p-value = 0.5\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.0397  0.0802\nsample estimates:\nmean difference \n         0.0202 \n```\n\n\n:::\n:::\n\n\n\n\n## Bayesian\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nm1 <- brm(RT ~ language + (1 | PP),\n    data = lies_long |> filter(veracity == \"L\"),\n    file = \"09_bayes_ttest\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 finished in 0.3 seconds.\nChain 3 finished in 0.3 seconds.\nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.4 seconds.\nChain 4 finished in 0.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 0.5 seconds.\n```\n\n\n:::\n\n```{.r .cell-code}\nfixef(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Estimate Est.Error    Q2.5  Q97.5\nIntercept   1.4704    0.0614  1.3470 1.5930\nlanguageE  -0.0195    0.0312 -0.0814 0.0427\n```\n\n\n:::\n:::\n\n\n\n\n---\n\n\n\n\n::: {.cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![](mcmc_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n\n::: {.cell-output-display}\n![](mcmc_files/figure-revealjs/unnamed-chunk-5-2.png){width=960}\n:::\n:::\n\n\n\n\n## Bayesian (cont'd)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm2 <- brm(\n    bf(RT ~ language + (1 |p| PP)),\n    data = lies_long |> filter(veracity == \"L\"),\n    family = lognormal(),\n    file = \"09_bayes_lognormal\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 finished in 0.4 seconds.\nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.6 seconds.\nChain 4 finished in 0.5 seconds.\nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.6 seconds.\nTotal execution time: 0.8 seconds.\n```\n\n\n:::\n\n```{.r .cell-code}\nfixef(m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Estimate Est.Error    Q2.5  Q97.5\nIntercept  0.32562    0.0419  0.2439 0.4075\nlanguageE -0.00733    0.0158 -0.0375 0.0241\n```\n\n\n:::\n:::\n\n\n\n\n---\n\n\n\n\n::: {.cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![](mcmc_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n\n::: {.cell-output-display}\n![](mcmc_files/figure-revealjs/unnamed-chunk-7-2.png){width=960}\n:::\n:::\n\n\n\n\n:::\n\n# Monte Carlo\n\n## Monte Carlo (MC) Methods\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n![](https://upload.wikimedia.org/wikipedia/commons/3/36/Real_Monte_Carlo_Casino.jpg)\n\n::: {.notes}\nImage credit: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Real_Monte_Carlo_Casino.jpg)\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n- 1930s and 40s: answer questions in nuclear physics not solvable with conventional mathematical methods\n    * Key figures: Stanislaw Ulam, John von Neumann, Nicholas Metropolis\n\n- Central element of the Manhattan Project in the development of the hydrogen bomb\n\n:::\n\n::::\n\n## MC With One Unknown\n\n`rbeta()`, `rnorm()`, `rbinom()`: generate values that imitate *independent samples* from known distributions\n\n- use *pseudorandom* numbers\n\nE.g., `rbeta(n, shape1 = 15, shape2 = 10)`{.r}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](mcmc_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=432}\n:::\n:::\n\n\n\n\n---\n\nWith a large number of draws (*S*),\n\n- sample density $\\to$ target distribution\n- most sample statistics (e.g., mean, quantiles) $\\to$ corresponding characteristics of the target density\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](mcmc_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=480}\n:::\n:::\n\n\n\n\n# Markov Chain Monte Carlo\n\n## MCMC\n\nMain problem in Bayesian: no way to draw independent samples from posterior\n$$\nP(\\theta \\mid y) = \\frac{\\mathrm{e}^{-(\\theta - 1 / 2)^2} \n                                     \\theta^y (1 - \\theta)^{n - y}}\n                       {\\int_0^1 \\mathrm{e}^{-(\\theta^* - 1 / 2)^2} \n                        {\\theta^*}^y (1 - {\\theta^*})^{n - y} d\\theta^*}\n$$\n\nMCMC: draw *dependent (correlated)* samples without evaluating the integral in the denominator\n\n---\n\nSome commonly used algorithms\n\n* The Metropolis algorithm (also called *random-walk* Metropolis)\n\n* Gibbs sampling (in BUGS, JAGS)\n\n* Hamiltonian Monte Carlo (and No-U-Turn sampler; in STAN)\n\n## The Metropolis Algorithm\n\n::: {.content-hidden unless-profile=\"class\"}\n![](/images/Metropolis_algorithm.png)\n:::\n\n## An Analogy {.smaller}\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n![](https://upload.wikimedia.org/wikipedia/commons/2/29/LA_districts_map.png){width=\"75%\"}\n\n:::\n\n::: {.column width=\"50%\"}\n\nYou have a task: tour all regions in LA county, and the time your spend on each region should be proportional to its popularity \n\nHowever, you don't know which region is the most popular\n\nEach day, you will decide whether to stay in the current region or move to a neighboring region\n\nYou have a tour guide that tells you whether region A is more or less popular than region B and by how much\n\n> How would you proceed?\n\n::: {.notes}\nImage credit: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:LA_districts_map.png)\n:::\n\n:::\n\n::::\n\n## Using the Metropolis Algorithm\n\n1. On each day, randomly select a new region\n2. If the *proposed* region is *more popular* than the current one, definitely go to the new region\n3. If the *proposed* region is *less popular* than the current one, go to the new region with  \n    $P(\\text{accept the new region}) = \\frac{\\text{proposed region popularity}}{\\text{current region popularity}}$\n    - E.g., by spinning a wheel\n\nIn the long run, distribution of time spent in each region = distribution of popularity of each region\n\n## Demonstration\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshiny::runGitHub(\"metropolis_demo\", \"marklhc\")\n```\n:::\n\n\n\n\n---\n\nData from LA Barometer (by the USC Dornsife Center for Economic and Social Research)\n\n338 first-gen immigrants, 86 used the metro in the previous year\n\nQuestion: \n\n> What proportion of first-gen immigrants uses the metro in a year?\n\n::: aside\nPress release: <https://dornsife.usc.edu/news/stories/3164/labarometer-mobility-in-los-angeles-survey/>\n:::\n\n## Analytic Posterior\n\nBeta(1.5, 2) prior $\\to$ Beta(87.5, 254) posterior\n\n1,000 independent draws from the posterior:\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mcmc_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n\n\n## With the Metropolis Algorithm\n\nProposal density: $N(0, 0.1)$; Starting value: $\\theta^{(1)} = 0.1$\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](mcmc_files/figure-revealjs/performance-metropolis-1.png){width=960}\n:::\n:::\n\n\n\n\n::: aside\nSee R code in the note\n:::\n\n---\n\nWith enough *iterations*, the Metropolis will simulate samples from the target distribution\n\nIt is *less efficient* than `rbeta()`{.r} because the draws are *dependent*\n\n. . .\n\n::: {.callout-tip}\n\n## Pros\n\n- does not require solving the integral\n- can use non-conjugate priors\n- easy to implement\n:::\n\n::: {.callout-warning}\n\n## Cons\n- not efficient; not scalable in complex models\n- require tuning the proposal SD; \n\n:::\n\n# MCMC Diagnostics\n\n## Markov Chain\n\nMarkov chain: a sequence of iterations, $\\{\\theta^{(1)}, \\theta^{(2)}, \\ldots, \\theta^{(S)}\\}$\n\n- the \"state\" $\\theta^{(s)}$ depends on $\\theta^{(s - 1)}$\n    * where to travel next depends on where the current region is\n\nBased on *ergodic* theorems, a well-behaved chain will reach a *stationary distribution*\n\n- after which, every draw is a sample from the stationary distribution\n\n::: {.content-hidden unless-profile=\"class\"}\n\n## Exercise\n\n1. Try using different proposal SD: 0.02, 0.1, 0.3, 1\n2. For each, draw 1,000 samples\n3. Does the sample distribution look like the target posterior?\n4. What is the relationship between proposal SD, acceptance rate, and autocorrelation?\n5. Which proposal SD seems to give the best effective sample size?\n\n:::\n\n## Warm-up\n\nIt takes a few to a few hundred thousand iterations for the chain to get to the stationary distribution\n\nTherefore, a common practice is to discard the first $S_\\text{warm-up}$ (e.g., first half of the) iterations\n\n- Also called *burn-in*\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mcmc_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n\n\n## When Can We Use MCMC Draws to Approximate the Posterior?\n\n1. The draws need to be *representative* of the posterior\n2. The draws contain sufficient information to *accurately* describe the posterior\n\n. . .\n\n### Tools\n\n- Trace plots/Rank histograms\n- $\\hat R$\n- Effective sample size (ESS)\n\n## Representativeness\n\nThe chain does not get stuck\n\n*Mixing*: multiple chains cross each other\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](mcmc_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n\n\n## Representativeness (cont'd)\n\nFor more robust diagnostics [@vehtari2021]\n\n- The rank histograms should look like uniform distributions\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mcmc_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n\n\n## Representativeness (cont'd)\n\n$$\n\\hat{R} = \\frac{\\text{Between-chain variance} + \\text{within-chain variance}}\n                 {\\text{within-chain variance}}\n$$\n\n- aka: *Gelman-Rubin statistic*, the *potential scale reduction factor*\n\n. . .\n\nWhen the chains converge, each should be exploring the same stationary distribution\n\n- No between-chain differences $\\Rightarrow$ $\\hat{R} \\to 1$\n- @vehtari2021 recommended $\\hat{R} < 1.01$ for convergence\n\n---\n\nIn the previous examples, \n\n- $\\hat R$ = 2.044 for the poor mixing graph\n- $\\hat R$ = 1.033 for the good mixing graph\n\n## Effective Sample Size (ESS)\n\nMCMC draws are dependent, so they contain less information for the target posterior distribution\n\n> What is the equivalent number of draws if the draws were independent?\n\n- E.g., ESS = 98.289 for the good mixing example\n    * Need ~5087.022 draws to get equal amount of information as 1,000 independent samples\n\n## Heuristics for ESS\n\n- ESS (bulk and tail) > 400 to interpret $\\hat R$ [@vehtari2021]\n- ESS > 1000 for stable summary of the posterior\n    * Kruschke (2015) recommended 10,000\n\n## Sample Convergence Paragraph\n\n\n\n\n\n\n\n\n\n> We used Markov Chain Monte Carlo (MCMC), specifically a Metropolis algorithm implemented in R, to approximate the posterior distribution of the model parameters. We used two chains, each with 10,000 draws. The first 5,000 draws in each chain were discarded as warm-ups. Trace plots of the posterior samples (Figure X) showed good mixing, and $\\hat R$ statistics (Vehtari et al., 2021) were < 1.01 for all model parameters, indicating good convergence for the MCMC chains. The effective sample sizes > 2376.931 for all model parameters, so the MCMC draws are sufficient for summarizing the posterior distributions.\n\n## Sample Results\n\n> The model estimated that 25.569% (posterior SD = 2.328%, 90% CI [21.813%, 29.467%]) of first-generation immigrants took the metro in the year 2019.\n\n## Things to Remember\n    \n- MCMC draws dependent/correlated samples to approximate a posterior distribution\n    * ESS < $S$\n- It needs warm-up iterations to reach a stationary distribution\n- Check for representativenes\n    * Trace/Rank plot and $\\hat{R}$\n- Need large ESS to describe the posterior accurately\n\n## References",
    "supporting": [
      "mcmc_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}