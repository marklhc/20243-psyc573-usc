{
  "hash": "021327b4d11b2d3c27bd63f1dbcf226b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Probability and Bayes Theorem\"\ndate: \"September 3, 2024\"\ndate-modified: last-modified\nformat: metropolis-revealjs\n---\n\n\n\n\n\n\n::: {.content-hidden unless-profile=\"class\"}\n\n##\n\n- HW 1\n- HW 2 posted\n- Week 4 classes\n- Week 2 overview\n\n## {background-image=\"https://upload.wikimedia.org/wikipedia/commons/2/28/Casino_Lights_In_Macau.jpg\"}\n\n::: {.notes}\nImage credit: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Casino_Lights_In_Macau.jpg)\n:::\n\n::::\n\n## History of Probability\n\n- Origin: To study gambling problems\n\n- A mathematical way to study uncertainty/randomness\n\n. . .\n\n::: {.callout}\n## Thought Experiment\n\nSomeone asks you to play a game. The person will flip a coin. You win $10 if it shows head, and lose $10 if it shows tail. Would you play?\n\n![](https://images.pexels.com/photos/14911424/pexels-photo-14911424.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1){width=\"33%\" fig-align=\"right\"}\n:::\n\n::: {.notes}\nImage credit: [Jonathan Borba](https://www.pexels.com/photo/close-up-of-cryptocurrency-coins-14911424/)\n:::\n\n##\n\n::: {.callout-note}\n\n## Kolmogorov axioms\nFor an event $A_i$ (e.g., getting a \"1\" from throwing a die)\n\n- $P(A_i) \\geq 0$  [All probabilities are non-negative]\n\n- $P(A_1 \\cup A_2 \\cup \\cdots) = 1$  [Union of all possibilities is 1]\n\n- $P(A_1) + P(A_2) = P(A_1 \\text{ or } A_2)$ for mutually exclusive $A_1$ and $A_2$ [Addition rule]\n:::\n\n## Throwing a Die With Six Faces\n\n![](https://upload.wikimedia.org/wikipedia/commons/e/ef/One_die.jpeg){.absolute top=100 right=100 width=\"100\" height=\"100\"}\n\n$A_1$ = getting a one, . . . $A_6$ = getting a six\n\n- $P(A_i) \\geq 0$\n- $P(\\text{the number is 1, 2, 3, 4, 5, or 6}) = 1$\n- $P(\\text{the number is 1 or 2}) = P(A_1) + P(A_2)$\n\nMutually exclusive: $A_1$ and $A_2$ cannot both be true\n\n::: {.notes}\nImage credit: Kcida10 at English Wikipedia, CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0>, via Wikimedia Commons\n:::\n\n# Interpretations of Probability\n\n## Ways to Interpret Probability\n\n- **Classical:** Counting rules\n\n- **Frequentist:** long-run relative frequency\n\n- **Subjectivist:** Rational belief\n\n::: aside\nNote: there are other paradigms to interpret probability. See <https://plato.stanford.edu/entries/probability-interpret/>\n:::\n\n## Classical Interpretation\n\n![](../docs/images/dice.png){fig-align=\"center\"}\n\n- Number of target outcomes / Number of possible \"indifferent\" outcomes\n    * E.g., Probability of getting \"1\" when throwing a die: 1 / 6\n\n## Frequentist Interpretation\n\n- Long-run relative frequency of an outcome\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n| Trial | Outcome |\n|:-----:|:-------:|\n|   1   |    2    |\n|   2   |    3    |\n|   3   |    1    |\n|   4   |    3    |\n|   5   |    1    |\n|   6   |    1    |\n|   7   |    5    |\n|   8   |    6    |\n|   9   |    3    |\n|  10   |    3    |\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"60%\"}\n\n\n\n::: {.cell layout-nrow=\"2\" layout-ncol=\"2\"}\n::: {.cell-output-display}\n![](probability_and_bayes_theorem_files/figure-revealjs/unnamed-chunk-3-1.png){width=100%}\n:::\n\n::: {.cell-output-display}\n![](probability_and_bayes_theorem_files/figure-revealjs/unnamed-chunk-3-2.png){width=100%}\n:::\n\n::: {.cell-output-display}\n![](probability_and_bayes_theorem_files/figure-revealjs/unnamed-chunk-3-3.png){width=100%}\n:::\n\n::: {.cell-output-display}\n![](probability_and_bayes_theorem_files/figure-revealjs/unnamed-chunk-3-4.png){width=100%}\n:::\n:::\n\n\n\n:::\n\n::::\n\n\n## Problem of the single case\n\nSome events cannot be repeated\n\n- Probability of Democrats/Republicans winning the 2024 election\n- Probability of the LA Chargers winning the 2024 Super Bowl\n\n. . .\n\nOr, probability that the null hypothesis is true\n\n. . .\n\n*For frequentist, probability is not meaningful for a single case*\n\n## Subjectivist Interpretation\n\n- State of one's mind; the belief of all outcomes\n    * Subjected to the constraints of:\n        * Axioms of probability\n        * That the person possessing the belief is rational\n\n. . .\n\n\n\n\n::: {.cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![](probability_and_bayes_theorem_files/figure-revealjs/unnamed-chunk-4-1.png){width=40%}\n:::\n\n::: {.cell-output-display}\n![](probability_and_bayes_theorem_files/figure-revealjs/unnamed-chunk-4-2.png){width=40%}\n:::\n:::\n\n\n\n\n## Describing a Subjective Belief\n\n- Assign a value for every possible outcome\n    * Not an easy task\n\n- Use a *probability distribution* to approximate the belief\n    * Usually by following some conventions\n    * Some distributions preferred for computational efficiency\n\n# Probability Distribution\n\n## Probability Distributions\n\n- Discrete outcome: Probability **mass**\n\n- Continuous outcome: Probability **density**\n\n\n\n\n::: {.cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![](probability_and_bayes_theorem_files/figure-revealjs/unnamed-chunk-5-1.png){width=90%}\n:::\n\n::: {.cell-output-display}\n![](probability_and_bayes_theorem_files/figure-revealjs/unnamed-chunk-5-2.png){width=90%}\n:::\n:::\n\n\n\n\n## Probability Density\n\n- If $X$ is continuous, the probability of $X$ having any particular value $\\to$ 0\n    * E.g., probability a person's height is 174.3689 cm\n\n. . .\n\nInstead, we obtain **probability density**: \n$$\nP(x_0) = \\lim_{\\Delta x \\to 0} \\frac{P(x_0 < X < x_0 + \\Delta x)}{\\Delta x}\n$$\n\n## Normal Probability Density\n\n::: {.panel-tabset}\n\n### Math\n\n$$\nP(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp\\left(-\\frac{1}{2}\\left[\\frac{x - \\mu}{\\sigma}\\right]^2\\right)\n$$\n\n### Plot\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](probability_and_bayes_theorem_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=67%}\n:::\n:::\n\n\n\n\n### R Code\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_normal_density <- function(x, mu, sigma) {\n    exp(- ((x - mu) / sigma) ^2 / 2) / (sigma * sqrt(2 * pi))\n}\n```\n:::\n\n\n\n\n:::\n\n## Some Commonly Used Distributions\n\n[![](https://upload.wikimedia.org/wikipedia/commons/6/69/Relationships_among_some_of_univariate_probability_distributions.jpg)](https://commons.wikimedia.org/wiki/File:Relationships_among_some_of_univariate_probability_distributions.jpg)\n\n::: {.notes}\nImage credit: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Relationships_among_some_of_univariate_probability_distributions.jpg)\n:::\n\n## Summarizing a Probability Distribution\n\n::: {.callout}\n\n## Central tendency\n\nThe center is usually the region of values with high plausibility\n\n- Mean, median, mode\n\n:::\n\n::: {.callout}\n\n## Dispersion\n\nHow concentrated the region with high plausibility is\n\n- Variance, standard deviation\n- Median absolute deviation (MAD)\n\n:::\n\n## Summarizing a Probability Distribution (cont'd)\n\n::: {.callout}\n\n## Interval\n\n- One-sided\n- Symmetric\n- Highest density interval (HDI)\n\n:::\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](probability_and_bayes_theorem_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=432}\n:::\n:::\n\n\n\n\n## Multiple Variables\n\n- Joint probability: $P(X, Y)$\n- Marginal probability: $P(X)$, $P(Y)$\n\n![](https://upload.wikimedia.org/wikipedia/commons/e/ef/One_die.jpeg){.absolute bottom=100 right=100 width=\"100\" height=\"100\"}\n\n|     | >= 4   | <= 3   | Marginal (odd/even) |\n|-----|--------|--------|:-----:|\n| odd | 1/6    | 2/6    | 3/6 |\n| even| 2/6    | 1/6    | 3/6 |\n| Marginal (>= 4 or <= 3) | 3/6  | 3/6  |  1  |\n\n## Multiple Continuous Variables\n\n- Left: Continuous $X$, Discrete $Y$\n- Right: Continuous $X$ and $Y$\n\n\n\n\n::: {.cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![](probability_and_bayes_theorem_files/figure-revealjs/unnamed-chunk-9-1.png){width=90%}\n:::\n\n::: {.cell-output-display}\n![](probability_and_bayes_theorem_files/figure-revealjs/unnamed-chunk-9-2.png){width=90%}\n:::\n:::\n\n\n\n\n::: {.notes}\nExample of Mixed continuous-discrete variables: $X$ = continuous outcome, $Y$ = binary treatment indicator\n:::\n\n## Conditional Probability\n\nKnowing the value of $B$, the relative plausibility of each value of outcome $A$\n\n$$\nP(A \\mid B_1) = \\frac{P(A, B_1)}{P(B_1)}\n$$\n\nE.g., P(Alzheimer's) vs. P(Alzheimer's | family history)\n\n##\n\nE.g., Knowing that the number is odd\n\n|              | >= 4     | <= 3     |\n|--------------|----------|----------|\n| odd          | <span style=\"color:red\">1/6</span>    | <span style=\"color:red\">2/6</span>    |\n| ~~even~~     | ~~2/6~~  | ~~1/6~~  |\n| Marginal (>= 4 or <= 3) | 3/6  | 3/6  |\n\n. . .\n\nConditional = Joint / Marginal\n\n|              | >= 4     | <= 3     |\n|--------------|----------|----------|\n| odd          | <span style=\"color:red\">1/6</span>    | <span style=\"color:red\">2/6</span>    |\n| Marginal (>= 4 or <= 3) | 3/6  | 3/6  |\n| Conditional (odd) | <span style=\"color:red\">(1/6)</span> / <span style=\"color:purple\">(3/6)</span> = 1/3 | <span style=\"color:red\">(1/6)</span> / <span style=\"color:purple\">(2/6)</span> = 2/3 |\n\n##\n\n### $P(A \\mid B) \\neq P(B \\mid A)$\n\n- $P$(number is six | even number) = 1 / 3\n\n- $P$(even number | number is six) = 1\n\n::: {.callout-tip}\n\n## Another example:\n\n$P$(road is wet | it rains) vs. $P$(it rains | road is wet)\n\n- Problem: Not considering other conditions leading to wet road: sprinkler, street cleaning, etc\n\nSometimes called the *confusion of the inverse* \n\n:::\n\n## Independence\n\n::: {.callout-important appearance=\"simple\"}\n$A$ and $B$ are independent if \n\n$$\nP(A \\mid B) = P(A)\n$$\n:::\n\n. . .\n\nE.g.,\n\n- $A$: A die shows five or more\n- $B$: A die shows an odd number\n\n. . .\n\nP(>= 5) = 1/3. P(>=5 | odd number) = ? P(>=5 | even number) = ?\n\nP(<= 5) = 2/3. P(<=5 | odd number) = ? P(>=5 | even number) = ?\n\n## Law of Total Probability\n\nFrom conditional $P(A \\mid B)$ to marginal $P(A)$\n\n- If $B_1, B_2, \\cdots, B_n$ are all possibilities for an event (so they add up to a probability of 1), then\n\n$$\n\\begin{align}\n    P(A) & = P(A, B_1) + P(A, B_2) + \\cdots + P(A, B_n)  \\\\\n         & = P(A \\mid B_1)P(B_1) + P(A \\mid B_2)P(B_2) + \\cdots + P(A \\mid B_n) P(B_n)  \\\\\n         & = \\sum_{k = 1}^n P(A \\mid B_k) P(B_k)\n\\end{align}\n$$\n\n![](../docs/images/total_probability.png){.absolute bottom=\"70\" right=\"70\" width=\"400\"}\n\n##\n\n::: {.callout-note}\n\n## Example\n\nConsider the use of a depression screening test for people with diabetes. For a person with depression, there is an 85% chance the test is positive. For a person without depression, there is a 28.4% chance the test is positive. Assume that 19.1% of people with diabetes have depression. If the test is given to 1,000 people with diabetes, around how many people will be tested positive? \n\n:::\n\n::: aside\nData source: https://doi.org/10.1016/s0165-0327(12)70004-6, https://doi.org/10.1371/journal.pone.0218512\n:::\n\n# Bayes Theorem\n\n\n\n\n\n\n\n\n\n::: {.content-hidden unless-profile=\"class\"}\n\n##\n\n- HW 1 Recap\n\n## Quiz\n\nIn a population, 20% of people carries a gene that makes them more prone to disease X. 10% of the people with the gene gets disease X, whereas only 5% of those without the gene gets disease X. What is the probability that a random person gets disease X?\n\n:::\n\n##\n\n::: {.callout-important}\n\n## Bayes Theorem\n\nGiven $P(A, B) = P(A \\mid B) P(B) = P(B \\mid A) P(A)$ (joint = conditional $\\times$ marginal)\n\n$$\nP(B \\mid A) = \\dfrac{P(A \\mid B) P(B)}{P(A)}\n$$\n\nWhich says how we can go from $P(A \\mid B)$ to $P(B \\mid A)$\n\n:::\n\n::: {.callout-important appearance=\"simple\"}\n\nConsider $B_i$ $(i = 1, \\ldots, n)$ as one of the many possible mutually exclusive events\n\n$$\n\\begin{aligned}\n  P(B_i \\mid A) & = \\frac{P(A \\mid B_i) P(B_i)}{P(A)}  \\\\\n             & = \\frac{P(A \\mid B_i) P(B_i)}{\\sum_{k = 1}^n P(A \\mid B_k)P(B_k)}\n\\end{aligned}\n$$\n\n:::\n\n## Example\n\nA police officer stops a driver *at random* and does a breathalyzer test for the driver. The breathalyzer is known to detect true drunkenness 100% of the time, but in **1%** of the cases, it gives a *false positive* when the driver is sober. We also know that in general, for every **1,000** drivers passing through that spot, **one** is driving drunk. Suppose that the breathalyzer shows positive for the driver. What is the probability that the driver is truly drunk?\n\n:::: {.content-hidden unless-profile=\"class\"}\n\n##\n\n:::: {.columns}\n\n::: {.columns width=\"50%\"}\n$P(\\text{positive} \\mid \\text{drunk}) = 1$  \n$P(\\text{positive} \\mid \\text{sober}) = 0.01$  \n:::\n\n::: {.columns width=\"50%\"}\n$P(\\text{drunk}) = 1 / 1000$  \n$P(\\text{sober}) = 999 / 1000$\n:::\n\n::::\n\n. . .\n\nUsing Bayes Theorem, \n\n$$\n\\begin{aligned}\n  & \\quad\\; P(\\text{drunk} \\mid \\text{positive})  \\\\\n  & = \\frac{P(\\text{positive} \\mid \\text{drunk}) P(\\text{drunk})}\n           {P(\\text{positive} \\mid \\text{drunk}) P(\\text{drunk}) + \n            P(\\text{positive} \\mid \\text{sober}) P(\\text{sober})}  \\\\\n  & = \\frac{1 \\times 0.001}{1 \\times 0.001 + 0.01 \\times 0.999} \\\\\n  & = 100 / 1099 \\approx 0.091\n\\end{aligned}\n$$\n\n---\n\nSo there is less than 10% chance that the driver is drunk even when the \nbreathalyzer shows positive.\n\n::: {.callout appearance=\"simple\"}\nA. Even with the breathalyzer showing positive, it is still very likely that the driver is not drunk\n:::\n\n::: {.callout appearance=\"simple\"}\nB. On the other hand, before the breathalyzer result, the person only has a 0.1% chance of being drunk. The breathalyzer result increases that probability to 9.1% (i.e., 91 times bigger)\n:::\n\n. . .\n\nBoth (A) and (B) are true. It just means that there is still much uncertainty after one positive test\n\n::: {.notes}\nHaving a second test may be helpful, assuming that what causes a false positive in the first test does not guarantee a false positive in the second test (otherwise, the second test is useless). That's one reason for not having consecutive tests too close in time.\n:::\n\n::::\n\n::: {visibility=\"hidden\"}\n\n## Gigerenzer (2004)\n\n$p$ value = $P$(data | hypothesis), not $P$(hypothesis | data)\n\n. . .\n\nConsider:\n\n- $H_0$: the person is sober (not drunk)\n- data: breathalyzer result\n\n$p$ = $P$(positive | sober) = 0.01 $\\rightarrow$ reject $H_0$ at .05 level\n\n. . .\n\nHowever, as we have seen, given that $P(H_0)$ is small, $P(H_0 \\mid \\text{data})$ is still small\n\n:::\n\n# Bayesian Data Analysis\n\n## Bayes Theorem in Data Analysis\n\n- Bayesian statistics\n    * more than applying Bayes theorem\n    * a way to quantify the plausibility of every possible value of some parameter $\\theta$\n        * E.g., population mean, regression coefficient, etc\n    * Goal: **update one's Belief about $\\theta$ based on the observed data $D$**\n\n## Going back to the example\n\nGoal: Find the probability that the person is drunk, given the test result\n\nParameter ($\\theta$): drunk (values: drunk, sober)\n\nData ($D$): test (possible values: positive, negative)\n\n. . .\n\nBayes theorem: $\\underbrace{P(\\theta \\mid D)}_{\\text{posterior}} = \\underbrace{P(D \\mid \\theta)}_{\\text{likelihood}} \\underbrace{P(\\theta)}_{\\text{prior}} / \\underbrace{P(D)}_{\\text{marginal}}$\n\n##\n\nUsually, the marginal is not given, so\n\n$$\nP(\\theta \\mid D) = \\frac{P(D \\mid \\theta)P(\\theta)}{\\sum_{\\theta^*} P(D \\mid \\theta^*)P(\\theta^*)}\n$$\n\n- $P(D)$ is also called *evidence*, or the *prior predictive distribution*\n    * E.g., probability of a positive test, regardless of the drunk status\n\n## Example 2\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshiny::runGitHub(\"plane_search\", \"marklhc\")\n```\n:::\n\n\n\n- Try choosing different priors. How does your choice affect the posterior?\n- Try adding more data. How does the number of data points affect the posterior?\n\n##\n\nThe posterior is a synthesis of two sources of information: prior and data (likelihood)\n\nGenerally speaking, a narrower distribution (i.e., smaller variance) means more/stronger information\n\n- Prior: narrower = more informative/strong\n- Likelihood: narrower = more data/more informative\n\n::: {.content-hidden unless-profile=\"class\"}\n\n## Q1\n\nThe posterior distribution describes the\n\nA. conditional probability of the parameters given the data\n\nB. joint probability of the parameters and the data\n\nC. conditional probability of the data given the parameters\n\nD. marginal probability of the data\n\n## Q2\n\nWhich of the following is a weak prior, compared to the other?\n\n\n\n::: {.cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![](probability_and_bayes_theorem_files/figure-revealjs/unnamed-chunk-12-1.png){width=432}\n:::\n\n::: {.cell-output-display}\n![](probability_and_bayes_theorem_files/figure-revealjs/unnamed-chunk-12-2.png){width=432}\n:::\n:::\n\n\n\n##\n\nExercise:\n\n- Shiny app with a parameter (fixed)\n- Ask students to formulate a prior distribution\n- Flip a coin, and compute the posterior by hand (with R)\n- Use the posterior as prior, flip again, and obtain the posterior again\n- Compare to use the original prior with two coin flips (both numbers and plots)\n- Flip 10 times, and show how the posterior change (using animation in `knitr`)\n:::\n\n## Priors\n\n*Prior beliefs used in data analysis must be admissible by a skeptical scientific audience (Kruschke, 2015, p. 115)*\n\n. . . \n\n- **Flat**, noninformative, vague\n- **Weakly informative**: common sense, logic\n- **Informative**: publicly agreed facts or theories\n\n\n\n::: {.cell layout-ncol=\"3\"}\n::: {.cell-output-display}\n![](probability_and_bayes_theorem_files/figure-revealjs/unnamed-chunk-13-1.png){width=95%}\n:::\n\n::: {.cell-output-display}\n![](probability_and_bayes_theorem_files/figure-revealjs/unnamed-chunk-13-2.png){width=95%}\n:::\n\n::: {.cell-output-display}\n![](probability_and_bayes_theorem_files/figure-revealjs/unnamed-chunk-13-3.png){width=95%}\n:::\n:::\n\n\n\n## Likelihood/Model/Data $P(D \\mid \\theta, M)$\n\n*Probability of observing the data **as a function of the parameter(s)***\n\n- Also written as $L(\\theta \\mid D)$ or $L(\\theta; D)$ to emphasize it is a function of $\\theta$\n- Also depends on a chosen model $M$: $P(D \\mid \\theta, M)$\n\n\n\n::: {.cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![](probability_and_bayes_theorem_files/figure-revealjs/unnamed-chunk-14-1.png){width=90%}\n:::\n\n::: {.cell-output-display}\n![](probability_and_bayes_theorem_files/figure-revealjs/unnamed-chunk-14-2.png){width=90%}\n:::\n:::\n\n\n\n## Likelihood of Multiple Data Points\n\n1. Given $D_1$, obtain *posterior* $P(\\theta \\mid D_1)$\n2. Use $P(\\theta \\mid D_1)$ as *prior*, given $D_2$, obtain posterior $P(\\theta \\mid D_1, D_2)$\n\nThe posterior is the same as getting $D_2$ first then $D_1$, or $D_1$ and $D_2$ together, if\n\n- **data-order invariance** is satisfied, which means\n- $D_1$ and $D_2$ are **exchangeable**\n\n##\n\n::: {.callout-important}\n\n## Exchangeability\n\nJoint distribution of the data does not depend on the order of the data\n\nE.g., $P(D_1, D_2, D_3) = P(D_2, D_3, D_1) = P(D_3, D_2, D_1)$\n\n:::\n\n. . .\n\nExample of non-exchangeable data:\n\n- First child = male, second = female vs. first = female, second = male\n- $D_1, D_2$ from School 1; $D_3, D_4$ from School 2 vs. $D_1, D_3$ from School 1; $D_2, D_4$ from School 2\n\n# Bernoulli Example\n\n## Coin Flipping: Binary Outcomes\n\nQ: Estimate the probability that a coin gives a head\n\n- $\\theta$: parameter, probability of a head\n\nFlip a coin, showing head\n\n- $y = 1$ for showing head\n\n## Multiple Binary Outcomes\n\n**Bernoulli model** is natural for binary outcomes\n\nAssume the flips are exchangeable given $\\theta$,\n$$\n\\begin{align}\nP(y_1, \\ldots, y_N \\mid \\theta) &= \\prod_{i = 1}^N P(y_i \\mid \\theta) \\\\\n&= \\theta^z (1 - \\theta)^{N - z}\n\\end{align}\n$$\n\n:::: {.columns}\n\n::: {.column width=45%}\n$z$ = # of heads; $N$ = # of flips\n:::\n\n::: {.column width=55%}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](probability_and_bayes_theorem_files/figure-revealjs/lik-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n:::\n\n::::\n\n## Posterior\n\n### Same posterior, two ways to think about it\n\n:::: {.columns}\n\n::: {.column width=50%}\n\n::: {.callout-note appearance=\"simple\"}\n\n## Prior belief, weighted by the likelihood\n\n$$\nP(\\theta \\mid y) \\propto \\underbrace{P(y \\mid \\theta)}_{\\text{weights}} P(\\theta)\n$$\n\n:::\n\n:::\n\n::: {.column width=50%}\n\n::: {.callout-note appearance=\"simple\"}\n\n## Likelihood, weighted by the strength of prior belief\n\n$$\nP(\\theta \\mid y) \\propto \\underbrace{P(\\theta)}_{\\text{weights}} P(\\theta \\mid y)\n$$\n\n:::\n\n:::\n\n::::\n\n## Grid Approximation\n\nSee Exercise 2\n\nDiscretize a continuous parameter into a finite number of discrete values\n\nFor example, with $\\theta$: [0, 1] $\\to$ [.05, .15, .25, ..., .95]\n\n\n\n::: {.cell layout-ncol=\"2\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](probability_and_bayes_theorem_files/figure-revealjs/unnamed-chunk-16-1.png){fig-align='center' width=90%}\n:::\n\n::: {.cell-output-display}\n![](probability_and_bayes_theorem_files/figure-revealjs/unnamed-chunk-16-2.png){fig-align='center' width=90%}\n:::\n:::\n\n\n\n\n\n# Criticism of Bayesian Methods\n\n## Criticism of \"Subjectivity\"\n\nMain controversy: subjectivity in choosing a prior\n\n- Two people with the same data can get different results because of different chosen priors\n\n::: {.callout-note}\n\n## Counters to the Subjectivity Criticism\n\n- With enough data, different priors hardly make a difference\n- Prior: just a way to express the degree of ignorance\n    * One can choose a weakly informative prior so that the Influence of subjective Belief is small\n\n:::\n\n##\n\n::: {.callout-note}\n\n## Counters to the Subjectivity Criticism 2\n    \nSubjectivity in choosing a prior is\n\n- Same as in choosing a model, which is also done in frequentist statistics\n- Relatively strong prior needs to be justified, \n    * Open to critique from other researchers\n- Inter-subjectivity $\\rightarrow$ Objectivity\n\n:::\n\n::: {.callout-note}\n\n## Counters to the Subjectivity Criticism 3\n    \nThe prior is a way to incorporate previous research efforts to accumulate scientific evidence\n\n> Why should we ignore all previous literature every time we conduct a new study?\n\n:::\n\n",
    "supporting": [
      "probability_and_bayes_theorem_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}