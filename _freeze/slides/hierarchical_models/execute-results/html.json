{
  "hash": "f0995ede6914730bd1d6cb5a8317236e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Hierarchical Models\"\ndate: \"September 17, 2024\"\ndate-modified: last-modified\nformat: metropolis-revealjs\n---\n\n\n\n\n\n\n# Therapeutic Touch Example (*N* = 28)\n\n## Data Points From One Person\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n![](https://c.pxhere.com/photos/6b/f4/protect_hands_energy_ecology_protection_sun_live_responsibility-1334144.jpg!d)\n\n$y$: whether the guess of which hand was hovered over was correct\n\n:::\n\n::: {.column width=\"60%\"}\n\nPerson S01\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|  y|s   |\n|--:|:---|\n|  1|S01 |\n|  0|S01 |\n|  0|S01 |\n|  0|S01 |\n|  0|S01 |\n|  0|S01 |\n|  0|S01 |\n|  0|S01 |\n|  0|S01 |\n|  0|S01 |\n\n\n:::\n:::\n\n\n\n\n:::\n\n::::\n\n## Binomial Model\n\nWe can use a Bernoulli model:\n$$\ny_i \\sim \\mathrm{Bern}(\\theta)\n$$\nfor $i = 1, \\ldots, N$\n\n. . .\n\nAssuming exchangeability given $\\theta$, more succint to write\n$$\nz \\sim \\mathrm{Bin}(N, \\theta)\n$$\nfor $z = \\sum_{i = 1}^N y_i$\n\n. . .\n\n- Bernoulli: Individual trial\n- Binomial: total count of \"1\"s\n\n---\n\nPrior: Beta(1, 1)\n\n1 success, 9 failures\n\nPosterior: Beta(2, 10)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](hierarchical_models_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n\n\n## Multiple People\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](hierarchical_models_files/figure-revealjs/unnamed-chunk-4-1.png){width=432}\n:::\n:::\n\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\nWe could repeat the binomial model for each of the 28 participants, to obtain posteriors for $\\theta_1$, $\\ldots$, $\\theta_{28}$\n\n:::\n\n::::\n\nBut . . . \n\n::: {.notes}\nWe'll continue the therapeutic touch example. To recap, we have 28 participants, each of them go through 10 trials to guess which of their hands was hovered above. The histogram shows the distribution of the proportion correct.\n:::\n\n. . .\n\n> Do we think our belief about $\\theta_1$ would inform our belief about $\\theta_2$, etc?\n\n. . .\n\nAfter all, human beings share 99.9% of genetic makeup\n\n## Three Positions of Pooling\n\n- No pooling: each individual is completely different; inference of $\\theta_1$ should be independent of $\\theta_2$, etc\n\n- Complete pooling: each individual is exactly the same; just one $\\theta$ instead of 28 $\\theta_j$'s\n\n- **Partial pooling**: each individual has something in common but also is somewhat different\n\n## No Pooling\n\n\n\n\n```{dot}\n//| fig-align: center\n//| fig-asp: 0.8\n//| out-width: 100%\ndigraph nopool {\n  layout=neato\n\n  node [penwidth = 0, fontname = \"Ubuntu\"]\n\n  # Person\n  th1 [pos = \"-1.2,1!\", label=<&theta;<SUB>1</SUB>>];\n  th2 [pos = \"-0.6,1!\", label=<&theta;<SUB>2</SUB>>]\n  th3 [pos = \"0,1!\", label=\"...\"]\n  th4 [pos = \"0.6,1!\", label=<&theta;<SUB>J - 1</SUB>>]\n  th5 [pos = \"1.2,1!\", label=<&theta;<SUB>J</SUB>>]\n\n  # Repeated measures\n  y1 [pos = \"-1.2,0!\", label=<y<SUB>1</SUB>>]\n  y2 [pos = \"-0.6,0!\", label=<y<SUB>2</SUB>>]\n  y3 [pos = \"0,0!\", label=\"...\"]\n  y4 [pos = \"0.6,0!\", label=<y<SUB>J - 1</SUB>>]\n  y5 [pos = \"1.2,0!\", label=<y<SUB>J</SUB>>]\n\n  # edges\n  edge [dir = \"none\"]\n  th1 -> y1\n  th2 -> y2\n  th4 -> y4\n  th5 -> y5\n}\n```\n\n\n\n\n## Complete Pooling\n\n\n\n\n```{dot}\n//| fig-align: center\n//| fig-asp: 0.8\n//| out-width: 100%\ndigraph completepool {\n  layout=neato\n\n  node [penwidth = 0, fontname = \"Ubuntu\"]\n\n  # Person\n  th [pos = \"0,1!\", label=<&theta;>];\n\n  # Repeated measures\n  y1 [pos = \"-1.2,0!\", label=<y<SUB>1</SUB>>]\n  y2 [pos = \"-0.6,0!\", label=<y<SUB>2</SUB>>]\n  y3 [pos = \"0,0!\", label=\"...\"]\n  y4 [pos = \"0.6,0!\", label=<y<SUB>J - 1</SUB>>]\n  y5 [pos = \"1.2,0!\", label=<y<SUB>J</SUB>>]\n\n  # edges\n  edge [dir = \"none\"]\n  th -> {y1; y2; y4; y5}\n}\n```\n\n\n\n\n## Partial Pooling\n\n\n\n\n```{dot}\n//| fig-align: center\n//| fig-asp: 0.5\n//| fig-height: 6\ndigraph partialpool {\n  layout=neato\n\n  node [penwidth = 0, fontname = \"Ubuntu\"]\n\n  # Common parameters\n  hy [pos = \"0,2!\", label=<&mu;, &kappa;>]\n\n  # Person\n  th1 [pos = \"-1.2,1!\", label=<&theta;<SUB>1</SUB>>];\n  th2 [pos = \"-0.6,1!\", label=<&theta;<SUB>2</SUB>>]\n  th3 [pos = \"0,1!\", label=\"...\"]\n  th4 [pos = \"0.6,1!\", label=<&theta;<SUB>J - 1</SUB>>]\n  th5 [pos = \"1.2,1!\", label=<&theta;<SUB>J</SUB>>]\n\n  # Repeated measures\n  y1 [pos = \"-1.2,0!\", label=<y<SUB>1</SUB>>]\n  y2 [pos = \"-0.6,0!\", label=<y<SUB>2</SUB>>]\n  y3 [pos = \"0,0!\", label=\"...\"]\n  y4 [pos = \"0.6,0!\", label=<y<SUB>J - 1</SUB>>]\n  y5 [pos = \"1.2,0!\", label=<y<SUB>J</SUB>>]\n\n  # edges\n  edge [dir = \"none\"]\n  hy -> {th1; th2; th4; th5;}\n  th1 -> y1\n  th2 -> y2\n  th4 -> y4\n  th5 -> y5\n}\n```\n\n\n\n\n## Partial Pooling in Hierarchical Models\n\nHierarchical Priors: $\\theta_j \\sim \\mathrm{Beta2}(\\mu, \\kappa)$\n\nBeta2: *reparameterized* Beta distribution\n\n- mean $\\mu = a / (a + b)$\n- concentration $\\kappa = a + b$\n\nExpresses the prior belief:\n\n> Individual $\\theta$s follow a common Beta distribution with mean $\\mu$ and concentration $\\kappa$\n\n## How to Choose $\\kappa$\n\nIf $\\kappa \\to \\infty$: everyone is the same; no individual differences (i.e., complete pooling)\n\nIf $\\kappa = 0$: everybody is different; nothing is shared (i.e., no pooling)\n\n. . .\n\nWe can fix a $\\kappa$ value based on our belief of how individuals are similar or different\n\n. . .\n\nA more Bayesian approach is to treat $\\kappa$ as an unknown, and use Bayesian inference to update our belief about $\\kappa$\n\n---\n\nGeneric prior by Kruschke (2015): $\\kappa$ $\\sim$ Gamma(0.01, 0.01) \n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](hierarchical_models_files/figure-revealjs/unnamed-chunk-8-1.png){width=480}\n:::\n:::\n\n\n\n\nSometimes you may want a stronger prior like Gamma(1, 1), if it is unrealistic to do no pooling\n\n## Full Model\n\nModel:\n$$\n\\begin{aligned}\n  z_j & \\sim \\mathrm{Bin}(N_j, \\theta_j) \\\\\n  \\theta_j & \\sim \\mathrm{Beta2}(\\mu, \\kappa)\n\\end{aligned}\n$$\nPrior:\n$$\n\\begin{aligned}\n  \\mu & \\sim \\mathrm{Beta}(1.5, 1.5) \\\\\n  \\kappa & \\sim \\mathrm{Gamma}(0.01, 0.01)\n\\end{aligned}\n$$\n\n::: {.notes}\nHere's our model. For each person j, the number of correct responses follows a binoial distribution in N equals 10 trials. The probability, or the ability to sense the experimenter's hand, is captured by theta j.\n\nNow, we assume the theta js come from a common Beta distribution. Beta here is parameterized with the common mean mu, and the concentration kappa that controls how much to pool.\n\nAs for the priors, because mu is the common mean of the thetas, it is again between 0 and 1. I set a weak Beta(1.5, 1.5) prior, which corresponds to half a success and half a failure. If you have stronger belief that the data mostly reflect random guessing, you can put something like Beta(5, 5).\n\nThe gamma prior for kappa is recommended by your text as a weak default prior.\n:::\n\n---\n\n\n\n\n::: {.cell output.var='hbin_mod'}\n\n```{.stan .cell-code}\ndata {\n  int<lower=0> J;  // number of clusters (e.g., studies, persons)\n  array[J] int y;  // number of \"1\"s in each cluster\n  array[J] int N;  // sample size for each cluster\n}\nparameters {\n  // cluster-specific probabilities\n  vector<lower=0, upper=1>[J] theta;\n  real<lower=0, upper=1> mu;  // overall mean probability\n  real<lower=0> kappa;        // overall concentration\n}\nmodel {\n  y ~ binomial(N, theta);  // each observation is binomial\n  // Priors\n  theta ~ beta_proportion(mu, kappa);\n  mu ~ beta(1.5, 1.5);      // weak prior\n  kappa ~ gamma(.1, .1);  // prior recommended by Kruschke\n}\ngenerated quantities {\n  // Prior and posterior predictive\n  real<lower=0, upper=1> prior_mu = beta_rng(1.5, 1.5);\n  real<lower=0> prior_kappa = gamma_rng(.1, .1);\n  vector<lower=0, upper=1>[J] prior_theta;\n  for (j in 1:J) {\n    prior_theta[j] = beta_proportion_rng(prior_mu, prior_kappa);\n  }\n  array[J] int prior_ytilde = binomial_rng(N, prior_theta);\n  // Posterior predictive\n  array[J] int ytilde = binomial_rng(N, theta);\n}\n```\n:::\n\n\n\n\n::: {.notes}\nHere is the Stan code. The inputs are J, the number of people, y, which is actually z in our model for the individual counts, but I use y just because y is usually the outcome in Stan. N is the number of trials per person, and here N[J] means the number of trials can be different across individuals.\n\nThe parameters and the model block pretty much follow the mathematical model. The beta_proportion() function is what I said Beta2 as the beta distribution with the mean and the concentration as the parameters.\n\nYou may want to pause here to make sure you understand the Stan code.\n:::\n\n---\n\n```r\nhbin_mod <- cmdstan_model(\"stan_code/hierarchical-binomial.stan\")\n```\n\n```r\ntt_fit <- hbin_mod$sample(\n    data = list(J = nrow(tt_agg),\n                y = tt_agg$y,\n                N = tt_agg$n,\n                prior_only = FALSE),\n    seed = 1716,  # for reproducibility\n    refresh = 1000\n)\n```\n\n## Posterior of Hyperparameters\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bayesplot)\ntt_fit$draws(c(\"mu\", \"kappa\")) |>\n    mcmc_dens()\n```\n\n::: {.cell-output-display}\n![](hierarchical_models_files/figure-revealjs/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\n::: {.notes}\nThe graphs show the posterior for mu and kappa. As you can see, the average probability of guessing correctly has most density between .4 and .5.\n\nFor kappa, the posterior has a pretty long tail, and the value of kappa being very large, like 100 or 200, is pretty likely. So this suggests the individuals may be pretty similar to each other.\n:::\n\n## Shrinkage\n\n\n\n\n::: {.cell layout-nrow=\"2\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](hierarchical_models_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=384}\n:::\n\n::: {.cell-output-display}\n![](hierarchical_models_files/figure-revealjs/unnamed-chunk-12-2.png){fig-align='center' width=384}\n:::\n:::\n\n\n\n\n::: {.notes}\nFrom the previous model, we get posterior distributions for all parameters, including mu, kappa, and 28 thetas. The first graph shows the posterior for theta for person 1. The red curve is the one without any pooling, so the distribution is purely based on the 10 trials for person 1. The blue curve, on the other hand, is much closer to .5 due to partial pooling. Because the posterior of kappa is pretty large, the posterior is pooled towards the grand mean, mu.\n\nFor the graph below, the posterior mean is close to .5 with or without partial pooling, but the distribution is narrower with partial pooling, which reflects a stronger belief. This is because, with partial pooling, the posterior distribution uses more information than just the 10 trials of person 15; it also borrows information from the other 27 individuals.\n:::\n\n## Multiple Comparisons?\n\nFrequentist: family-wise error rate depends on the number of intended contrasts\n\n::: {.notes}\n\nOne advantage of the hierarchical model is it is a solution to the multiple comparison problem. In frequentist analysis, if you have multiple groups, and you want to test each contrast, you will need to consider family-wise error rate, and do something like Bonferroni corrections.\n:::\n\n. . .\n\nBayesian: only one posterior; hierarchical priors already express the possibility that groups are the same\n\n::: {.notes}\nThe Bayesian alternative is to do a hierarchial model with partial pooling. With Bayesian, you have one posterior distribution, which is the joint distribution of all parameters. And the use of a common distribution of the thetas already assigns some probability to the prior belief that the groups are the same.\n:::\n\n. . .\n\n> Thus, Bayesian hierarchical model \"completely solves the multiple comparisons problem.\"^[see <https://statmodeling.stat.columbia.edu/2016/08/22/bayesian-inference-completely-solves-the-multiple-comparisons-problem/>]\n\n::: {.notes}\nTherefore, with a hierarchical model, you can obtain the posterior of the difference of any groups, without worrying about how many comparisons you have conducted. You can read more in the sources listed here.\n:::\n\n# Hierarchical Normal Model\n\n::: {.notes}\nIn this video, we'll talk about another Bayesian hierarchical model, the hierarchical normal model.\n:::\n\n## Effect of coaching on SAT-V\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|School | Treatment.Effect.Estimate| Standard.Error|\n|:------|-------------------------:|--------------:|\n|A      |                        28|             15|\n|B      |                         8|             10|\n|C      |                        -3|             16|\n|D      |                         7|             11|\n|E      |                        -1|              9|\n|F      |                         1|             11|\n|G      |                        18|             10|\n|H      |                        12|             18|\n\n\n:::\n:::\n\n\n\n\n::: {.notes}\nThe data come from the 1980s when scholars were debating the effect of coaching on standardized tests. The test of interest is the SAT verbal subtest. The note contains more description of it.\n\nThe analysis will be on the secondary data from eight schools, from school A to school H. Each schools conducts its own randomized trial. The middle column shows the treatment effect estimate for the effect of coaching. For example, for school A, we see that students with coaching outperformed students without coaching by 28 points. However, for schools C and E, the effects were smaller and negative. \n\nFinally, in the last column, we have the standard error of the treatment effect for each school, based on a t-test. As you know, the smaller the standard error, the less uncertainty we have on the treatment effect.\n:::\n\n---\n\nModel:\n$$\n\\begin{aligned}\n  d_j & \\sim N(\\theta_j, s_j) \\\\\n  \\theta_j & \\sim N(\\mu, \\tau)\n\\end{aligned}\n$$\nPrior:\n$$\n\\begin{aligned}\n  \\mu & \\sim N(0, 100) \\\\\n  \\tau & \\sim t^+_4(0, 100)\n\\end{aligned}\n$$\n\n::: {.notes}\nWe can use the same idea of partial pooling for this data. The idea is that, while the effect of coaching may be different across schools, there should be some similarity of the schools. Like if you are a school official, if you hear all other schools found coaching to be increasing performance, you'd probably expect coaching works for the students in your school as well.\n\nSo the logic is the same; but instead of a binomial outcome, we have something like a continuous outcome in treatment effect, so we model the treatment effect, which I call d here, by a normal distribution. Note that d here is the sample difference between the treatment and the control group; because the sample difference is not the true treatment effect, we assume d j is normally distributed with a mean theta j, where theta j is the true treatment effect for school j. s j here is the standard error of the treatment effect, the third column in the data. It reflects the degree of uncertainty in the sample treatment effect d. \n\nNext, we have the theta js coming from a common normal distribution, with mean mu, and standard deviation tau. So like kappa in the previous model, tau here controls how much to pool. If tau is small, it means the thetas are very similar; if tau is large, it means the thetas are very different.\n:::\n\n---\n\n\n\n\n::: {.cell output.var='hnorm_mod'}\n\n```{.stan .cell-code}\ndata {\n  int<lower=0> J;            // number of schools \n  vector[J] y;               // estimated treatment effects\n  vector<lower=0>[J] sigma;  // s.e. of effect estimates \n}\nparameters {\n  real mu;                   // overall mean\n  real<lower=0> tau;         // between-school SD\n  vector[J] eta;             // standardized deviation (z score)\n}\ntransformed parameters {\n  vector[J] theta;\n  theta = mu + tau * eta;    // non-centered parameterization\n}\nmodel {\n  eta ~ std_normal();        // same as eta ~ normal(0, 1);\n  y ~ normal(theta, sigma);\n  // priors\n  mu ~ normal(0, 100);\n  tau ~ student_t(4, 0, 100);\n}\n```\n:::\n\n\n\n\n## Individual-School Treatment Effects\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](hierarchical_models_files/figure-revealjs/unnamed-chunk-17-1.png){fig-align='center' width=480}\n:::\n:::\n\n\n\n\n## Prediction Interval\n\nPosterior distribution of the true effect size of a new study, $\\tilde \\theta$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](hierarchical_models_files/figure-revealjs/unnamed-chunk-18-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\n\n::: aside\nSee <https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.12> for an introductory paper on random-effect meta-analysis\n:::",
    "supporting": [
      "hierarchical_models_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}